{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [6]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T14:15:03.466661Z",
     "iopub.status.busy": "2021-02-10T14:15:03.466090Z",
     "iopub.status.idle": "2021-02-10T14:15:03.518058Z",
     "shell.execute_reply": "2021-02-10T14:15:03.517386Z"
    },
    "papermill": {
     "duration": 0.084087,
     "end_time": "2021-02-10T14:15:03.518240",
     "exception": false,
     "start_time": "2021-02-10T14:15:03.434153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "        function toggle(id) {\n",
       "            el = document.getElementById(id);\n",
       "            el.style.display = el.style.display === \"none\" ? \"block\" : \"none\";\n",
       "        }\n",
       "    </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "This notebook is compatible with this base image version (user-0.22.3)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from edc import check_compatibility\n",
    "check_compatibility(\"user-0.22.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033105,
     "end_time": "2021-02-10T14:15:03.579788",
     "exception": false,
     "start_time": "2021-02-10T14:15:03.546683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How To: Land-Use-Land-Cover Prediction for Slovenia - using the Batch Processing API\n",
    "\n",
    "\n",
    "This notebook is based on the [Land-User-Land-Cover Prediction example](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html) described in the [EOLearn documentation](https://eo-learn.readthedocs.io/en/latest/index.html). The example workflow uses `EOLearn` to construct a machine learning pipeline for predicting the land use / land cover for the region of the Republic of Slovenia. The import of satellite images to train a model which is then used for the predictions is performed using an [EOTask](https://eo-learn.readthedocs.io/en/latest/eotasks.html) based on Sentinel Hub's [process API](https://docs.sentinel-hub.com/api/latest/api/process/). While this approach is efficient for most applications, querying large volumes of data and performing the processing steps locally can complicate scaling to larger areas (country or continent-wide analysis for example).\n",
    "\n",
    "In this notebook, we present an alternative workflow, where the acquisition of the satellite data, processing of derived products and resampling over a fixed timestep is performed with Sentinel Hub services. The process allows for much faster processing times over large areas, reduced costs and the user needs less computational resources to process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027445,
     "end_time": "2021-02-10T14:15:03.634757",
     "exception": false,
     "start_time": "2021-02-10T14:15:03.607312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Before you start\n",
    "#### Requirements\n",
    "\n",
    "In order to run the example you will need an EDC Sentinel Hub API access (Enterprise plan) to access satellite data and an [Amazon Bucket](https://aws.amazon.com/s3/) to save the outputs to.\n",
    "\n",
    "To configure the notebook to work with your Sentinel Hub account you can use the `edc` configurator as shown below. You will also need to specify your Amazon Bucket access credentials (user id and secret for [programmatic access](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys)).\n",
    "\n",
    "\n",
    "#### Input data\n",
    "\n",
    "You can access the example input data from the [Github repository](https://github.com/sentinel-hub/eo-learn/tree/master/examples/batch-processing/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027591,
     "end_time": "2021-02-10T14:15:03.691510",
     "exception": false,
     "start_time": "2021-02-10T14:15:03.663919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is organised in 2 main sections:\n",
    "\n",
    "- **Part 1** is dedicated to creating and running the Batch Process with [Sentinel Hub Python package](https://sentinelhub-py.readthedocs.io/en/latest/).\n",
    "\n",
    "- **Part 2** focusses on converting the results obtained in *Part 1* to [EOPatches](https://eo-learn.readthedocs.io/en/latest/examples/core/CoreOverview.html#EOPatch), the format used in [EOLearn](https://eo-learn.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "- **Part 3** shows how to integrate the workflow into the [LULC pipeline](https://github.com/sentinel-hub/eo-learn/blob/master/examples/land-cover-map/SI_LULC_pipeline.ipynb) to predict LULC using machine learning algorithms. \n",
    "\n",
    "Let’s start!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T14:15:03.752363Z",
     "iopub.status.busy": "2021-02-10T14:15:03.751812Z",
     "iopub.status.idle": "2021-02-10T14:15:06.734531Z",
     "shell.execute_reply": "2021-02-10T14:15:06.733946Z"
    },
    "papermill": {
     "duration": 3.015741,
     "end_time": "2021-02-10T14:15:06.734666",
     "exception": false,
     "start_time": "2021-02-10T14:15:03.718925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basics of Python data handling and visualization\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "\n",
    "# Sentinel Hub\n",
    "from sentinelhub import (SentinelHubBatch, SentinelHubRequest, Geometry, CRS, BBox, DataCollection, MimeType, SHConfig,\n",
    "                         BatchSplitter)\n",
    "\n",
    "# EDC\n",
    "from edc import setup_environment_variables\n",
    "\n",
    "# EOLearn\n",
    "from eolearn.core import (EOPatch, EOExecutor, OverwritePermission, FeatureType, SaveTask, LoadTask,\n",
    "                          LinearWorkflow)\n",
    "from eolearn.geometry import VectorToRaster, ErosionTask, PointSamplingTask\n",
    "\n",
    "# Amazon\n",
    "import boto3\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path, PosixPath\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from fs_s3fs import S3FS\n",
    "from fs import open_fs\n",
    "from aenum import MultiValueEnum\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027526,
     "end_time": "2021-02-10T14:15:06.789824",
     "exception": false,
     "start_time": "2021-02-10T14:15:06.762298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 1: create and run the Batch process\n",
    "\n",
    "Run `Sentinel Hub Batch Processing` to request data for an area of interest, and download the satellite products (bands and derived products) to an Amazon S3 Bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027532,
     "end_time": "2021-02-10T14:15:06.844813",
     "exception": false,
     "start_time": "2021-02-10T14:15:06.817281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First set up Sentinel Hub Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T14:15:06.905124Z",
     "iopub.status.busy": "2021-02-10T14:15:06.904539Z",
     "iopub.status.idle": "2021-02-10T14:15:06.908760Z",
     "shell.execute_reply": "2021-02-10T14:15:06.908311Z"
    },
    "papermill": {
     "duration": 0.035509,
     "end_time": "2021-02-10T14:15:06.908861",
     "exception": false,
     "start_time": "2021-02-10T14:15:06.873352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "API credentials have automatically been injected for your active subscriptions.  \n",
       "The following environment variables are now available:\n",
       "* `SH_CLIENT_ID`, `SH_CLIENT_NAME`, `SH_CLIENT_SECRET`, `SH_INSTANCE_ID`\n",
       "\n",
       "The following additional environment variables have been loaded from `~/custom.env`:\n",
       "* `AWS_BUCKET`\n",
       "* `DAPA_URL`\n",
       "* `DB_HOST`, `DB_NAME`, `DB_PASSWORD`, `DB_USER`\n",
       "* `OGC_EDC_URL`\n",
       "* `REFERENCE_DATA`\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup SH services access\n",
    "setup_environment_variables()\n",
    "\n",
    "CLIENT_ID = %env SH_CLIENT_ID\n",
    "CLIENT_SECRET = %env SH_CLIENT_SECRET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028363,
     "end_time": "2021-02-10T14:15:06.965032",
     "exception": false,
     "start_time": "2021-02-10T14:15:06.936669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Specify Amazon S3 bucket client ID and secret (See **Requirements** above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T14:15:07.027993Z",
     "iopub.status.busy": "2021-02-10T14:15:07.027458Z",
     "iopub.status.idle": "2021-02-10T14:15:07.029135Z",
     "shell.execute_reply": "2021-02-10T14:15:07.029495Z"
    },
    "papermill": {
     "duration": 0.032807,
     "end_time": "2021-02-10T14:15:07.029616",
     "exception": false,
     "start_time": "2021-02-10T14:15:06.996809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "AWS_ID = \"aws-client-id\"\n",
    "AWS_SECRET = \"aws-client-secret\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031996,
     "end_time": "2021-02-10T14:15:07.089235",
     "exception": false,
     "start_time": "2021-02-10T14:15:07.057239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sentinel Hub authentification\n",
    "\n",
    "In the following cell, we will create an instance of ```sentinelhub.SHConfig``` for the Sentinel Hub services. The token is only valid for 1 hour. If you get an message of type ` accessToken signature expired` later in the workflow, just re-run the next cell. See more [information here](https://docs.sentinel-hub.com/api/latest/api/overview/authentication/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T14:15:07.149279Z",
     "iopub.status.busy": "2021-02-10T14:15:07.148725Z",
     "iopub.status.idle": "2021-02-10T14:15:07.151343Z",
     "shell.execute_reply": "2021-02-10T14:15:07.150810Z"
    },
    "papermill": {
     "duration": 0.033558,
     "end_time": "2021-02-10T14:15:07.151453",
     "exception": false,
     "start_time": "2021-02-10T14:15:07.117895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up credentials for use with batch\n",
    "config = SHConfig()\n",
    "\n",
    "if CLIENT_ID and CLIENT_SECRET:\n",
    "    config.sh_client_id = CLIENT_ID\n",
    "    config.sh_client_secret = CLIENT_SECRET\n",
    "else:\n",
    "    config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028079,
     "end_time": "2021-02-10T14:15:07.207263",
     "exception": false,
     "start_time": "2021-02-10T14:15:07.179184",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Define the Area-of-Interest (AOI):\n",
    "\n",
    "Although the Sentinel Hub Batch processing is designed for large areas, running this example Notebook for the entire Republic of Slovenia would be costly (in terms of storage and processing units used). To make the example reproducible, we will run the example for a smaller region: the administrative boundaries of the capital, Ljubljana.\n",
    "\n",
    "\n",
    "- A geographical shape of Slovenia was taken from [this webpage](http://alas.matf.bg.ac.rs/~mi09109/svn.html). The region of Ljubljana was extracted using QGIS and saved as a new geojson. In the following cell the `geojson` is imported and a buffer of 500 m is applied to it (to make sure to cover the entire extent). The shape `svn_border.geojson` is available [here](https://github.com/sentinel-hub/eo-learn/blob/master/examples/batch-processing/data/ljubljana.geojson).\n",
    "\n",
    "- Because the Batch Processing API can't process shapes with too many coordinates (up to 1500 points are supported), a simplified polygon covering the extent of the geographical shape is created (using `simplify`), and the coordinates extracted. In the current example, this step is optional, but was included for demonstration purposes.\n",
    "\n",
    "- The shape is split into smaller tiles by the batch process (see further down)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028866,
     "end_time": "2021-02-10T14:15:07.265373",
     "exception": false,
     "start_time": "2021-02-10T14:15:07.236507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.1.1 Import data, plot AOI, and check the points count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T14:15:07.329943Z",
     "iopub.status.busy": "2021-02-10T14:15:07.328313Z",
     "iopub.status.idle": "2021-02-10T14:15:07.495981Z",
     "shell.execute_reply": "2021-02-10T14:15:07.495426Z"
    },
    "papermill": {
     "duration": 0.202325,
     "end_time": "2021-02-10T14:15:07.496138",
     "exception": true,
     "start_time": "2021-02-10T14:15:07.293813",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "data/ljubljana.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: data/ljubljana.geojson: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-58b9ae591296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load geojson file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcountry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_DATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ljubljana.geojson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Apply a 500m buffer to the shape (indicate a specific feature with [i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/eurodatacube-0.22.3/lib/python3.8/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/eurodatacube-0.22.3/lib/python3.8/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/eurodatacube-0.22.3/lib/python3.8/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0m\u001b[1;32m    254\u001b[0m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[1;32m    255\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/eurodatacube-0.22.3/lib/python3.8/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: data/ljubljana.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Folder where data for running the notebook is stored\n",
    "INPUT_DATA = Path(\"./data/\") \n",
    "\n",
    "# Load geojson file\n",
    "country = gpd.read_file(INPUT_DATA.joinpath('ljubljana.geojson'))\n",
    "\n",
    "# Apply a 500m buffer to the shape (indicate a specific feature with [i])\n",
    "country_buffer = country[country.geometry.name][0].buffer(500)\n",
    "\n",
    "# Get the country's shape in polygon format\n",
    "country_shape = country.geometry.values[-1]\n",
    "\n",
    "# Plot country\n",
    "country.plot()\n",
    "plt.axis('off')\n",
    "\n",
    "# Print size and points count\n",
    "print('Dimension of the area is {0:.0f} x {1:.0f} m2'.format(country_shape.bounds[2] - country_shape.bounds[0],\n",
    "                                                             country_shape.bounds[3] - country_shape.bounds[1]))\n",
    "print('Points count after buffer: ', len(list(country_buffer.exterior.coords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 1.1.2 Get simplified bands and plot geographical extent  that will be used in requests\n",
    "**Note**: Make sure the points count does not exceed 1500 points after simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the simplified shape of the country\n",
    "country_sim = country_buffer.simplify(100) #unit in meter\n",
    "\n",
    "# Check the amount of points forming the polygon\n",
    "print('Points count: ',len(list(country_sim.exterior.coords)))\n",
    "\n",
    "# Plot shape\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.plot(*country_sim.exterior.xy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Use Batch to fetch Sentinel-2 data and resample to even timesteps\n",
    "\n",
    "Now that the area of interest is defined, we will set up the parameters necessary to run the `Batch` process, and go through the different steps to acquire the data.\n",
    "\n",
    "We will collect Sentinel-2 data and retrieve the following products:\n",
    "\n",
    " - L1C custom list of bands [B02, B03, B04, B08, B11, B12], which corresponds to [B, G, R, NIR, SWIR1, SWIR2] wavelengths.\n",
    "\n",
    "- Calculated NDVI, NDWI, and NDBI information\n",
    "\n",
    "- A mask of validity, based on acquired data from Sentinel and cloud coverage. Valid pixel is if:\n",
    "    - IS_DATA == True\n",
    "    - CLOUD_MASK == 0 (1 indicates cloudy pixels and 255 indicates `NO_DATA`)\n",
    "    \n",
    "All returned products will be returned for specific time intervals between 2 dates. If no data is present on the interval date, the service returns a linear interpolation between the previous and following valid data. The `Batch` request merges the [EOLearn data download step](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#Define-the-workflow) and [training data preparation step](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#5.-Prepare-the-training-data) into one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 1.2.1 Prepare Batch input parameters\n",
    "\n",
    "Here we set the start and end date of the time interval that we want to query, as well as a time step in days for which data will be returned.\n",
    "\n",
    "We also specify the name of the Amazon S3 Bucket that was created and parameterised as shown earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Date parameters: for this example, we will get data from early April to late October.\n",
    "START = datetime.date(2019, 4, 1)\n",
    "END = datetime.date(2019, 10, 31)\n",
    "\n",
    "INTERVAL = 15  # Interval for date resampling in days\n",
    "\n",
    "# Calculate list of dates for the resampling\n",
    "date_iterator = START\n",
    "timestamps = []\n",
    "while date_iterator < END:\n",
    "    timestamps.append(date_iterator)\n",
    "    date_iterator = date_iterator + datetime.timedelta(days=INTERVAL)\n",
    "    \n",
    "# Amazon bucket name\n",
    "aws_bucket_name = \"my-amazon-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Below, we define the evalscript that will perform all the work on Sentinel Hub servers. The script fetches the images, calculates the indices (NDVI, NDWI, and NDBI), and resamples the data to the defined interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evalscript = \"\"\"\n",
    "//VERSION=3\n",
    "\n",
    "// Calculate number of bands needed for all intervals\n",
    "// Initialise dates and interval\n",
    "// Beware: in JS months are 0 indexed\n",
    "var start_date = new Date(2019, 3, 1, 0, 0, 0);\n",
    "var end_date = new Date(2019, 9, 31, 0, 0, 0);\n",
    "var sampled_dates = sample_timestamps(start_date, end_date, 15, 'day').map(d => withoutTime(d));\n",
    "var nb_bands = sampled_dates.length;\n",
    "var n_valid = 0;\n",
    "var n_all = 0;\n",
    "\n",
    "function interval_search(x, arr) {\n",
    "  let start_idx = 0,  end_idx = arr.length - 2;\n",
    "\n",
    "  // Iterate while start not meets end\n",
    "  while (start_idx <= end_idx) {\n",
    "    // Find the mid index\n",
    "    let mid_idx = (start_idx + end_idx) >> 1;\n",
    "\n",
    "    // If element is present at mid, return True\n",
    "    if (arr[mid_idx] <= x && x < arr[mid_idx + 1]) {\n",
    "      return mid_idx;\n",
    "    }\n",
    "    // Else look in left or right half accordingly\n",
    "    else if (arr[mid_idx + 1] <= x) start_idx = mid_idx + 1;\n",
    "    else end_idx = mid_idx - 1;\n",
    "  }\n",
    "  if (x == arr[arr.length-1]){\n",
    "    return arr.length-2;\n",
    "  }\n",
    "\n",
    "  return undefined;\n",
    "}\n",
    "\n",
    "function linearInterpolation(x, x0, y0, x1, y1, no_data_value=NaN) {\n",
    "  if (x < x0 || x > x1) {\n",
    "    return no_data_value;\n",
    "  }\n",
    "  var a = (y1 - y0) / (x1 - x0);\n",
    "  var b = -a * x0 + y0;\n",
    "  return a * x + b;\n",
    "}\n",
    "\n",
    "function lininterp(x_arr, xp_arr, fp_arr, no_data_value=NaN) {\n",
    "  results = [];\n",
    "  data_mask = [];\n",
    "  xp_arr_idx = 0;\n",
    "  for(var i=0; i<x_arr.length; i++){\n",
    "    var x = x_arr[i];\n",
    "    n_all+=1;\n",
    "    interval = interval_search(x, xp_arr);\n",
    "    if (interval === undefined) {\n",
    "      data_mask.push(0);\n",
    "      results.push(no_data_value);\n",
    "      continue;\n",
    "    }\n",
    "    data_mask.push(1);\n",
    "    n_valid+=1;\n",
    "    results.push(\n",
    "      linearInterpolation(\n",
    "        x,\n",
    "        xp_arr[interval],\n",
    "        fp_arr[interval],\n",
    "        xp_arr[interval+1],\n",
    "        fp_arr[interval+1], \n",
    "        no_data_value\n",
    "      )\n",
    "    );\n",
    "  }\n",
    "\n",
    "  return [results, data_mask];\n",
    "}\n",
    "\n",
    "\n",
    "function interpolated_index(index_a, index_b){\n",
    "  // Calculates the index for all bands in array\n",
    "  var index_data = [];\n",
    "  for (var i = 0; i < index_a.length; i++){\n",
    "     // UINT index returned\n",
    "     let ind = (index_a[i] - index_b[i]) / (index_a[i] + index_b[i]);\n",
    "     index_data.push(ind * 10000 + 10000);\n",
    "  }\n",
    "  \n",
    "  return index_data\n",
    "}\n",
    "\n",
    "\n",
    "function increase(original_date, period, period_unit){\n",
    "    date = new Date(original_date)\n",
    "    switch(period_unit){\n",
    "        case 'millisecond':\n",
    "            return new Date(date.setMilliseconds(date.getMilliseconds()+period));\n",
    "        case 'second':\n",
    "            return new Date(date.setSeconds(date.getSeconds()+period));\n",
    "        case 'minute':\n",
    "            return new Date(date.setMinutes(date.getMinutes()+period));\n",
    "        case 'hour':\n",
    "            return new Date(date.setHours(date.getHours()+period));\n",
    "        case 'day':\n",
    "            return new Date(date.setDate(date.getDate()+period));\n",
    "        case 'month':\n",
    "            return new Date(date.setMonth(date.getMonth()+period));\n",
    "        default:\n",
    "            return undefined\n",
    "    }\n",
    "}\n",
    "\n",
    "function sample_timestamps(start, end, period, period_unit) {\n",
    "    var cDate = new Date(start);\n",
    "    var sampled_dates = []\n",
    "    while (cDate < end) {\n",
    "        sampled_dates.push(cDate);\n",
    "        cDate = increase(cDate, period, period_unit);\n",
    "    }\n",
    "    return sampled_dates;\n",
    "}\n",
    "\n",
    "function is_valid(smp){\n",
    "  // Check if the sample is valid (i.e. contains no clouds or snow)\n",
    "  let clm = smp.CLM;\n",
    "  let dm = smp.dataMask;\n",
    "\n",
    "  if (clm === 1 || clm === 255) {\n",
    "        return false;\n",
    "  } else if (dm !=1 ) {\n",
    "        return false;\n",
    "  } else {\n",
    "  return true;\n",
    "  }\n",
    "}\n",
    "\n",
    "function withoutTime(intime){\n",
    "  // Return date without time\n",
    "  intime.setHours(0, 0, 0, 0);\n",
    "  return intime;\n",
    "}\n",
    "\n",
    "\n",
    "// Sentinel Hub functions\n",
    "function setup() {\n",
    "  // Setup input/output parameters\n",
    "    return {\n",
    "        input: [{\n",
    "            bands: [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"B12\", \"CLM\", \"dataMask\"],\n",
    "            units: \"DN\"\n",
    "        }],\n",
    "      output: [\n",
    "          {id: \"B02\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"B03\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"B04\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"B08\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"B11\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"B12\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"NDVI\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"NDWI\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"NDBI\", bands: nb_bands, sampleType: SampleType.UINT16},\n",
    "          {id: \"data_mask\", bands: nb_bands, sampleType: SampleType.UINT8}\n",
    "      ],\n",
    "    mosaicking: \"ORBIT\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "// Evaluate pixels in the bands\n",
    "function evaluatePixel(samples, scenes) {\n",
    "  \n",
    "  // Initialise arrays\n",
    "  var valid_samples = {'B02':[], 'B03':[], 'B04':[], 'B08':[], 'B11':[], 'B12':[]}; \n",
    "  \n",
    "  var valid_dates = []\n",
    "  // Loop over samples. \n",
    "  for (var i = samples.length-1; i >= 0; i--){\n",
    "      if (is_valid(samples[i])) {\n",
    "        valid_dates.push(withoutTime(new Date(scenes[i].date)));\n",
    "        valid_samples['B02'].push(samples[i].B02);\n",
    "        valid_samples['B03'].push(samples[i].B03);\n",
    "        valid_samples['B04'].push(samples[i].B04);\n",
    "        valid_samples['B08'].push(samples[i].B08);\n",
    "        valid_samples['B11'].push(samples[i].B11);\n",
    "        valid_samples['B12'].push(samples[i].B12);\n",
    "      }\n",
    "  }\n",
    "  \n",
    "  var [b02_interpolated, b02_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B02'], 0);\n",
    "  var [b03_interpolated, b03_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B03'], 0);\n",
    "  var [b04_interpolated, b04_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B04'], 0);\n",
    "  var [b08_interpolated, b08_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B08'], 0);\n",
    "  var [b11_interpolated, b11_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B11'], 0);\n",
    "  var [b12_interpolated, b12_dm] = lininterp(sampled_dates, valid_dates, valid_samples['B12'], 0);\n",
    "\n",
    "  // Calculate indices and return optimised for UINT16 format (will need unpacking)\n",
    "  var ndvi = interpolated_index(b08_interpolated, b04_interpolated);\n",
    "  var ndwi = interpolated_index(b03_interpolated, b08_interpolated);\n",
    "  var ndbi = interpolated_index(b11_interpolated, b08_interpolated);\n",
    "  \n",
    "  // Return all arrays\n",
    "  return {\n",
    "            B02: b02_interpolated,\n",
    "            B03: b03_interpolated,\n",
    "            B04: b04_interpolated,\n",
    "            B08: b08_interpolated,\n",
    "            B11: b11_interpolated,\n",
    "            B12: b12_interpolated,\n",
    "            NDVI: ndvi,\n",
    "            NDWI: ndwi,\n",
    "            NDBI: ndbi,\n",
    "            data_mask: b02_dm\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Next, we set the parameters passed to the ```SentinelHubRequest()``` and the ```SentinelHubBatch()``` to run the batch processing request.\n",
    "\n",
    "The following parameters are specified:\n",
    "\n",
    "- **evalscript**: the evalscript that was written above.\n",
    "\n",
    "- **geometry**: the geometry parameters representing the area of interest.\n",
    "\n",
    "    The input type should be ```class 'shapely.geometry.polygon.Polygon'``` returned from [the “geometry” column](https://geopandas.org/data_structures.html) (```gdf[gdf.geometry.name][i]```).\n",
    "    \n",
    "- **crs**: the code corresponding to the projection parameters of the input data. \n",
    "\n",
    "    See the [input format](https://sentinelhub-py.readthedocs.io/en/latest/constants.html?highlight=SentinelHub.CRS#sentinelhub.constants.CRS) of the ```CRS``` helper for constant or enter in the following format: ```\"EPSG: 3035\"```.\n",
    "    \n",
    "- **datasource**: the parameters for the data to query.\n",
    "\n",
    "    See the most commonly used [predefined datacollections](https://sentinelhub-py.readthedocs.io/en/latest/_modules/sentinelhub/data_collections.html#DataCollection) or [define a new data collection](https://sentinelhub-py.readthedocs.io/en/latest/examples/data_collections.html?highlight=DataCollection.define#Define-a-new-data-collection)\n",
    "\n",
    "- **time_interval**: the start and end dates set the time period over which to fetch the data.\n",
    "\n",
    "- **mosaickingOrder**: Choose from ```\"mostRecent\"```, ```\"leastRecent\"```, and ```\"leastCC\"``` (more [info](https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l1c/#mosaickingorder)).\n",
    "\n",
    "- **upsampling/downsampling**: Choose from ```\"NEAREST\"```, ```\"BILINEAR\"```, and ```\"BICUBIC\"``` ([examples for Sentinel-2](https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l1c/#processing-options)).\n",
    "\n",
    "- **output_type**: Use ```MimeType``` to select a data type of the output response returned by the evalscript (more [info](https://sentinelhub-py.readthedocs.io/en/latest/_modules/sentinelhub/constants.html#MimeType)).\n",
    "\n",
    "- **tilingGridId** and **resolution**: these parameters parameters allow to set the grid and resolution of the images returned. To know which paramters to set, refer to the [dedicated documentation](https://docs.sentinel-hub.com/api/latest/api/batch/#tiling-grids). Here we return: \n",
    "\n",
    "    `tilingGridId` = 1, since less computation power is required for 10km grid. Larger tiling grids require more computational ressources.\n",
    "    \n",
    "    `resolution` = 10, which means our images will have a 10m resolution\n",
    "    \n",
    "- **buffer**: (Optional) A [buffer](https://sentinelhub-py.readthedocs.io/en/latest/sentinelhub_batch.html?highlight=SentinelHubBatch.tiling_grid#sentinelhub.sentinelhub_batch.SentinelHubBatch.tiling_grid) around each tile can be defined. \n",
    "    \n",
    "    Set in the following format: ```(int,int)``` or ```None```.\n",
    "    \n",
    "- **bucketName**: the [configured](https://docs.sentinel-hub.com/api/latest/api/batch/#aws-s3-bucket-settings) Amazon S3 Bucket\n",
    "- **description**: A personalised description for the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Evalscript = evalscript\n",
    "geometry = country_sim\n",
    "crs = CRS.UTM_32N\n",
    "datasource = DataCollection.SENTINEL2_L1C\n",
    "time_interval = (START, END)\n",
    "mosaickingOrder = \"mostRecent\"\n",
    "maxCloudCoverage = 1.0\n",
    "upsampling = \"NEAREST\"\n",
    "downsampling = \"NEAREST\"\n",
    "output_type = MimeType.TIFF \n",
    "tilingGridId = 1 \n",
    "resolution = 10 \n",
    "buffer = None\n",
    "bucketName = aws_bucket_name \n",
    "description = \"Slovenia LULC data example\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_shp = SentinelHubRequest(\n",
    "    evalscript=Evalscript,\n",
    "    input_data=[\n",
    "        SentinelHubRequest.input_data(\n",
    "            data_collection=datasource,\n",
    "            time_interval=time_interval,\n",
    "            mosaicking_order=mosaickingOrder,\n",
    "            maxcc=maxCloudCoverage,\n",
    "            upsampling=upsampling,\n",
    "            downsampling=downsampling,\n",
    "        )\n",
    "    ],\n",
    "    responses=[\n",
    "        SentinelHubRequest.output_response(\"B02\", output_type),\n",
    "        SentinelHubRequest.output_response(\"B03\", output_type),\n",
    "        SentinelHubRequest.output_response(\"B04\", output_type),\n",
    "        SentinelHubRequest.output_response(\"B08\", output_type),\n",
    "        SentinelHubRequest.output_response(\"B11\", output_type),\n",
    "        SentinelHubRequest.output_response(\"B12\", output_type),\n",
    "        SentinelHubRequest.output_response(\"NDVI\", output_type),\n",
    "        SentinelHubRequest.output_response(\"NDWI\", output_type),\n",
    "        SentinelHubRequest.output_response(\"NDBI\", output_type),\n",
    "        SentinelHubRequest.output_response(\"data_mask\", output_type)\n",
    "    ],\n",
    "    geometry=Geometry(geometry, crs=crs),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "batch_request = SentinelHubBatch.create(\n",
    "    request_shp,\n",
    "    tiling_grid=SentinelHubBatch.tiling_grid(\n",
    "        grid_id=tilingGridId,\n",
    "        resolution=resolution,\n",
    "        buffer=buffer\n",
    "    ),\n",
    "    bucket_name=bucketName,\n",
    "    description=description,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 1.2.2 Run the Batch request\n",
    "\n",
    "Once all the parameters are set we are ready to run the `Batch` process. The batch processing API comes with the set of REST APIs which support the execution of various workflows. A diagram of the statuses of a batch processing request is located [here](https://docs.sentinel-hub.com/api/latest/api/batch/#workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Start the request and return status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_request.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Extract request id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_id  = batch_request.info['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Re-initialize the request with its request id (Optional)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reinit = SentinelHubBatch(request_id,config=config)\n",
    "#reinit.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Show request status\n",
    "\n",
    "In the cell below, we will check the status of the batch processing request. Because Batch Processing API is an asynchronous REST service, we can execute this cell all along the process to verify the progress of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_request.update_info()\n",
    "batch_request.info[\"status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Additional analysis \n",
    "\n",
    "When executing the following cell:\n",
    "\n",
    "- the status of the request changes to ANALYSING,\n",
    "- the evalscript is validated,\n",
    "- a list of required tiles is created, and\n",
    "- the request's cost is estimated (i.e. the estimated number of processing units (PU) needed for the requested processing).\n",
    "- After the analysis is finished the status of the request changes to ANALYSIS_DONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_request.start_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Note: At this point you can run the **Show request status** cell again to check the status of the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Estimate number of processing units**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "processing_units = batch_request.info['valueEstimate']\n",
    "\n",
    "print(f'Running this batch job will require approximately {processing_units:.4f} processing units')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Check tile definitions**\n",
    "\n",
    "we can check information about all tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tile_info in batch_request.iter_tiles():\n",
    "    print(tile_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Optionally, we can request information about a single tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify a tile ID\n",
    "TILE_ID = '2558718'\n",
    "\n",
    "batch_request.get_tile(TILE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### START\n",
    "\n",
    "Once you happy with the request parameters and have noted the number of PU consumed (by checking the *Show request status*), you can launch the processing.\n",
    "\n",
    "**Note**: The processing time for this step could be up to an hour or more depending on the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_request.start_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Cancel the request\n",
    "- When the job is running we can decide at any time to cancel it. Results that have already been produced will remain on the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#batch_request.cancel_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Note: At this point you can run the **Show request status** cell again to check the status of the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Show request status\n",
    "\n",
    "In the cell below, we will check the status of the batch processing request. Because Batch Processing API is an asynchronous REST service, we can execute this cell all along the process to verify the progress of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_request.update_info()\n",
    "batch_request.info[\"status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Visual tracking of the Batch request\n",
    "\n",
    "The following cells allow us to track the progress of the Batch request by plotting the status of the tiles. To visually track the progress of the Batch request, we can keep executing the Plotting command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_batch_splitter(splitter):\n",
    "    \"\"\" Plots tiles and area geometry from a splitter class\n",
    "    \"\"\"\n",
    "    gdf = gpd.GeoDataFrame({\n",
    "        'status': [info['status'] for info in splitter.get_info_list()],\n",
    "        'geometry': splitter.get_bbox_list()\n",
    "    }, crs=\"EPSG:4326\")\n",
    "    gdf = gdf.to_crs(splitter.crs.pyproj_crs())\n",
    "    ax = gdf.plot(column='status', legend=True, figsize=(10, 10))\n",
    "\n",
    "    area_series = gpd.GeoSeries(\n",
    "        [splitter.get_area_shape()],\n",
    "        crs=splitter.crs.pyproj_crs()\n",
    "    )\n",
    "    \n",
    "    area_series.plot(ax=ax, facecolor='none', edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitter = BatchSplitter(batch_request=batch_request)\n",
    "plot_batch_splitter(splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### What if some of my tiles fail?\n",
    "\n",
    "It can happen that some of the tiles are not processed, and appear with the status `FAILED` in the plot above. We can use the function defined below to restart the processing for those specific tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Re-run the processing for all the tiles that failed\n",
    "batch_request.restart_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Alternatively, we can re-run processing only for a single tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify an ID of a tile that failed\n",
    "FAILED_TILE_ID = ''\n",
    "\n",
    "batch_request.reprocess_tile(FAILED_TILE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Part 2: convert Batch process outputs to EOPatches\n",
    "\n",
    "When the Batch request in **Part 1** has finished running, we should now have the data located in the specified Bucket. In **Part 2** we will focus on converting the results to EoPatches and download the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Set parameters for conversion \n",
    "\n",
    "First we specify a few parameters for converting the Batch results to EOLearn Patches.\n",
    "\n",
    "If you want to pick up the Notebook after having run the Batch request in an other session, you will need to specify a few parameters that were already defined in **Part 1**: for that uncomment and run the next cell. If you have just executed the cells in **Part 1**, you can skip the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Date parameters\n",
    "# START = datetime.date(2019, 4, 1)\n",
    "# END = datetime.date(2019, 10, 31)\n",
    "\n",
    "# INTERVAL = 15  # Interval for date resampling in days\n",
    "\n",
    "# # Calculate list of dates for the resampling\n",
    "# date_iterator = START\n",
    "# timestamps = []\n",
    "# while date_iterator < END:\n",
    "#     timestamps.append(date_iterator)\n",
    "#     date_iterator = date_iterator + datetime.timedelta(days=INTERVAL)\n",
    "    \n",
    "# #Amazon client ID and secret\n",
    "# AWS_ID = \"aws-client-id\"\n",
    "# AWS_SECRET = \"aws-client-secret\"\n",
    "    \n",
    "# # Amazon bucket name\n",
    "# aws_bucket_name = \"your-bucket-name\"\n",
    "\n",
    "# # Request ID (from the Batch processing)\n",
    "# request_id = \"f3e6140e-1728-4215-921d-488552667bab\"\n",
    "\n",
    "# #Configuration\n",
    "# config = SHConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Next, we will set the bands that we want to import into the `EOPatches`. \n",
    "\n",
    "In this example we use: \n",
    "   - B02, B03, B04, B08, B11, B12 bands from Sentinel-2\n",
    "   \n",
    "   - NDVI, NDWI, NDBI indices\n",
    "   \n",
    "   - Valid data band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "band_names = ['B02.tif', 'B03.tif', 'B04.tif', 'B08.tif', 'B11.tif','B12.tif',\n",
    "              'NDVI.tif', 'NDWI.tif', 'NDBI.tif']\n",
    "is_data_band = 'data_mask.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Finally, let's specify the folder to which we will save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output path\n",
    "save_folder = \"./results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We have defined a class to convert the Batch Request to `EOPatches` for ease of use below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImportFromAWS:\n",
    "    \"\"\"Import EoPatches from a Batch request.\n",
    "    \n",
    "    This class regroups all the methods to transform the saved results of a batch request\n",
    "    located in an Amazon Bucket to EOPatches locally.\n",
    "    \n",
    "    :param aws_key: Amazon client ID\n",
    "    :type aws_key: str\n",
    "    :param aws_secret: Amazon secret\n",
    "    :type aws_secret: str\n",
    "    :param aws_bucket: Amazon bucket name where the batch results were written\n",
    "    :type aws_bucket: str\n",
    "    :param request_id: Batch request id\n",
    "    :type request_id: str \n",
    "    param bands: List of bands that should be considered in the import\n",
    "    :type bands: list \n",
    "    param timestamps: List of timestamps for resampled data\n",
    "    :type timestamps: list\n",
    "    param is_data: Name of the IS_DATA band from Batch\n",
    "    :type is_data: str, optional\n",
    "    :param out_path: Path to folder where EOPatches will be saved\n",
    "    :type out_path: str, optional \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, aws_key, aws_secret, aws_bucket, request_id, bands, timestamps,\n",
    "                 is_data=None, out_path=\"./\"):\n",
    "        \"\"\"Constructor method.        \n",
    "        \"\"\"\n",
    "        # Path to save the EoPatches\n",
    "        self.out_path = Path(out_path)\n",
    "            \n",
    "        # Make directory\n",
    "        self.out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # AWS credentials\n",
    "        if (aws_key == None or aws_secret == None):\n",
    "            raise ValueError(\"No Amazon credential provided!\")\n",
    "            \n",
    "        # Create Amazon session and settings\n",
    "        self.session = boto3.Session(aws_access_key_id=aws_key, \n",
    "                                     aws_secret_access_key=aws_secret,\n",
    "                                    ) \n",
    "        self.s3_resource = self.session.resource('s3')\n",
    "        self.s3_client = self.session.client('s3')\n",
    "        self.bucket_name = aws_bucket\n",
    "        self.bucket = self.s3_resource.Bucket(aws_bucket)\n",
    "        \n",
    "        # Set up filesytem for Amazon Bucket\n",
    "        self.s3fs = S3FS(aws_bucket,\n",
    "                         dir_path=request_id,\n",
    "                         aws_access_key_id=aws_key,\n",
    "                         aws_secret_access_key=aws_secret)\n",
    "           \n",
    "        # Request id\n",
    "        self.request_id = request_id\n",
    "        \n",
    "        # Information about data and metadata\n",
    "        self.is_data = is_data\n",
    "        self.bands = bands\n",
    "        self.timestamps = [x.isoformat() for x in timestamps]\n",
    "\n",
    "\n",
    "    @staticmethod   \n",
    "    def _open_band(AWS_session, bucket, request_id, folder, bandname):\n",
    "        \"\"\"Open Band.\n",
    "        \n",
    "        Using Rasterio, open a band in an Amazon S3 Bucket, and return bbox and data.\n",
    "        \n",
    "        :param session: Amazon boto session\n",
    "        :type session: boto3.Session \n",
    "        :param bucket: Amazon bucket name\n",
    "        :type bucket: str\n",
    "        :param request_id: Name of the Batch request ID\n",
    "        :type request_id: str\n",
    "        :param folder: Name of the folder in which to query data\n",
    "        :type folder: str\n",
    "        :param bandname: Filename\n",
    "        :type bandname: str \n",
    "        \"\"\"\n",
    "        # Open with Rasterio\n",
    "        with rasterio.Env(rasterio.session.AWSSession(AWS_session)) as env:\n",
    "            s3_url = f's3://{bucket}/{request_id}/{folder}/{bandname}'\n",
    "            with rasterio.open(s3_url) as source:\n",
    "\n",
    "                    bbox = BBox(source.bounds, CRS(source.crs.to_epsg()))\n",
    "                    data = source.read()\n",
    "        return data, bbox\n",
    "    \n",
    "    \n",
    "    @staticmethod   \n",
    "    def _open_local_band(band_path):\n",
    "        \"\"\"Open Local Band.\n",
    "        \n",
    "        Using Rasterio, open a band in an Amazon S3 Bucket, and return bbox and data.\n",
    "        \n",
    "        :param session: Amazon boto session\n",
    "        :type session: boto3.Session \n",
    "        :param bucket: Amazon bucket name\n",
    "        :type bucket: str\n",
    "        :param request_id: Name of the Batch request ID\n",
    "        :type request_id: str\n",
    "        :param folder: Name of the folder in which to query data\n",
    "        :type folder: str\n",
    "        :param bandname: Filename\n",
    "        :type bandname: str \n",
    "        \"\"\"\n",
    "        # Open with Rasterio\n",
    "        with rasterio.open(band_path) as source:\n",
    "            bbox = BBox(source.bounds, CRS(source.crs.to_epsg()))\n",
    "            data = source.read()\n",
    "\n",
    "        return (data, bbox)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convert_DN(band, in_band):\n",
    "        \"\"\"Convert DN to values.\n",
    "        \n",
    "        It is cheaper to return INT data from SH Hub. In the evalscript a coefficient is applied\n",
    "        to the data to return it as INT, and here it needs to be unpacked. This function (for now)\n",
    "        considers only 2 ways of saving data: bands starting with B or indices not starting with B.\n",
    "        \n",
    "        :param band: Band name to convert\n",
    "        :type band: str \n",
    "        :param in_band: Numpy array to convert\n",
    "        :type in_band: np.array \n",
    "        :return: The input numpy array converted depending on band name\n",
    "        :rtype: np.array\n",
    "        \"\"\"\n",
    "        if band.startswith(\"B\"):\n",
    "            outband = in_band / 10000.\n",
    "        else:\n",
    "            outband = (in_band - 10000.) / 10000.\n",
    "\n",
    "        return outband\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _eopatch_import(folder_system, folders, bucket, bands, timestamps, out_path, local=False,\n",
    "                        save_local=True, is_data=None, request_id=None, bucket_name=None, session=None):\n",
    "        \"\"\"Import EOPatches from local data.\n",
    "        \n",
    "        After having downloaded the bucket contents to a local folder, extract the EOPatches.\n",
    "        \n",
    "        :param folder_system: a fs sytem object containing base directory (S3 or local)\n",
    "        :type folder_system: s3fs.S3FS or s3fs.core.S3FileSystem\n",
    "        :param folders: List of folders to search through\n",
    "        :type folders: list \n",
    "        :param bucket: boto3 resource Bucket bucket, see :class:`ImportFromAWS`\n",
    "        :type bucket: boto3.S3.Bucket(name)\n",
    "        :param bands: List of bands to process\n",
    "        :type bands: list\n",
    "        :param timestamps: List of timestamps corresponding to the bands\n",
    "        :type timestamps: list\n",
    "        :param out_path: Output folder where the EOPatches are saved\n",
    "        :type out_path: str\n",
    "        :param local: Set whether the folders are local or on Amazon S3\n",
    "        :type local: bool, optional\n",
    "        :param save_local: Set whether the EOPatches are saved to local or on Amazon S3\n",
    "        :type save_local: bool, optional\n",
    "        :param is_data: Name of band containing information of validity of data\n",
    "        :type is_data: str, optional\n",
    "        :param request_id: When importing from a Amazon bucket, folder name (from Batch)\n",
    "        :type request_id: str, optional\n",
    "        :param bucket_name: When importing from a Amazon bucket, bucket name\n",
    "        :type bucket_name: str, optional\n",
    "        :param session: When importing from a Amazon bucket, boto3 S3 session\n",
    "        :type session: boto3.S3.session, optional\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialise counter\n",
    "        nb_patch = 0\n",
    "        \n",
    "        # Ignore json file\n",
    "        folders = [x for x in folders if not x.endswith(\"json\")]\n",
    "        \n",
    "        # Loop over the folders which will become patches\n",
    "        for fold in tqdm(folders):\n",
    "          \n",
    "            # Export switch (in case the bands are missing in a patch)\n",
    "            export = True\n",
    "\n",
    "            # Empty EOPatch to be filled\n",
    "            eo_patch = EOPatch()\n",
    "\n",
    "            # List bands in folder\n",
    "            current_bands = folder_system.listdir(fold)\n",
    "            \n",
    "            # Check if the bands in the folder correspond to the bands wanted\n",
    "            if any(elem in current_bands for elem in bands):\n",
    "                all_bands = {}\n",
    "                # Loop through bands\n",
    "                for band in bands:\n",
    "                    # Different opening method depending on local or not\n",
    "                    if local:\n",
    "                        band_data, data_bbox = ImportFromAWS._open_local_band(Path(\n",
    "                            folder_system.getsyspath(\"\")).joinpath(fold, band))\n",
    "                    else:\n",
    "                        band_data, data_bbox = ImportFromAWS._open_band(session, \n",
    "                                                                        bucket_name, \n",
    "                                                                        request_id,\n",
    "                                                                        fold, band)\n",
    "                    # Convert band to reflectance or index and save\n",
    "                    all_bands[band] = ImportFromAWS._convert_DN(band, band_data)\n",
    "\n",
    "                     # Get IS_DATA\n",
    "                if is_data != None and is_data in current_bands:\n",
    "                    # Different opening method depending on local or not\n",
    "                    if local:\n",
    "                        is_data_array, _ = ImportFromAWS._open_local_band(Path(\n",
    "                                folder_system.getsyspath(\"\")).joinpath(fold, is_data))\n",
    "                    else:\n",
    "                        is_data_array, _ = ImportFromAWS._open_band(session, \n",
    "                                                                    bucket_name, \n",
    "                                                                    request_id,\n",
    "                                                                    fold, is_data)\n",
    "            # If the bands don't match, skip the patch\n",
    "            else:\n",
    "                print(f\"Bands don't match for Patch {fold}... skipping...\")\n",
    "                export = False\n",
    "\n",
    "            # if not flag to skip the band, continue\n",
    "            if export:\n",
    "                # Add bbox to patch\n",
    "                eo_patch.bbox = data_bbox\n",
    "\n",
    "                # Add bands to patch\n",
    "                eo_patch.data['FEATURES'] = np.stack([all_bands[x] for x in all_bands.keys()], axis=-1)\n",
    "                \n",
    "                # Add is data mask to patch\n",
    "                eo_patch.mask[\"IS_DATA\"] = np.expand_dims(is_data_array, axis=-1)\n",
    "\n",
    "                # Add metadata\n",
    "                eo_patch.meta_info[\"time_interval\"] = (START.isoformat(), END.isoformat())\n",
    "                eo_patch.meta_info[\"band_list\"] = [x.split('.')[0] for x in bands]\n",
    "\n",
    "                # Add timestamps\n",
    "                eo_patch.timestamp = timestamps\n",
    "\n",
    "                # Save EOPatch (either on local or on Bucket)\n",
    "                if save_local:\n",
    "                    save_path = str(Path(out_path).joinpath(\"EOPatches\"))\n",
    "                else:\n",
    "                    save_path = f\"s3://{bucket_name}/EOPatches\"\n",
    "\n",
    "                eo_patch.save(f\"{save_path}/eopatch_{nb_patch}\",\n",
    "                              overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "\n",
    "                # Increase counter\n",
    "                nb_patch += 1\n",
    "\n",
    "\n",
    "    def convert_to_EOPatches(self, local_bucket=None, save_locally=True):\n",
    "        \"\"\"Import of EOPatches.\n",
    "        \n",
    "        This function converts the contents of an Amazon bucket that is\n",
    "        stored locally (after download using e.g. `download_S3_bucket`) or \n",
    "        stored on Amazon S3 to EOPatches.\n",
    "        \n",
    "        :param local_bucket: A local folder containing contents of a Bucket\n",
    "        :type local_bucket: str, optional\n",
    "        \"\"\"\n",
    "        \n",
    "        if local_bucket is None:\n",
    "            # Open Bucket as folder system\n",
    "            folder_sys = self.s3fs\n",
    "            # Set local flag to False\n",
    "            local = False\n",
    "        else:\n",
    "            # Open local folder system\n",
    "            folder_sys = open_fs(local_bucket)\n",
    "            #Set local flag to True\n",
    "            local = True\n",
    "\n",
    "        # Get subfolders in the local folder\n",
    "        folders = folder_sys.listdir(\"/\")\n",
    "        \n",
    "        # Run the import\n",
    "        self._eopatch_import(folder_sys, folders, self.bucket, self.bands, self.timestamps,\n",
    "                             self.out_path, local=local, save_local=save_locally, is_data=self.is_data,\n",
    "                             request_id=self.request_id, bucket_name=self.bucket_name, session=self.session)\n",
    "\n",
    "\n",
    "    def download_from_bucket(self, obj_id=\"S3\"):\n",
    "        \"\"\"Download S3 Bucket.\n",
    "        \n",
    "        Downloads the results of a batch request stored in an Amazon\n",
    "        bucket, based on the request ID of the class.\n",
    "\n",
    "        \"\"\"\n",
    "        # Check type\n",
    "        if obj_id not in [\"S3\", \"EO\"]:\n",
    "            raise ValueError(\"Not a correct object ID to download: choose S3 or EO\")\n",
    "\n",
    "        # Check if a folder named after the reques ID already exists in the output folder\n",
    "        if obj_id == \"S3\":\n",
    "            path = self.request_id\n",
    "        else:\n",
    "            path = \"EOPatches\"\n",
    "            \n",
    "        if self.out_path.joinpath(path).is_dir():\n",
    "                raise ValueError(\"Bucket folder already exists on local path! Delete and try again.\")\n",
    "        \n",
    "        # Download S3/EO bucket\n",
    "        print(\"Downloading bucket contents\")\n",
    "        \n",
    "        keys = []\n",
    "        dirs = []\n",
    "        next_token = ''\n",
    "        base_kwargs = {\n",
    "            'Bucket':self.bucket_name,\n",
    "            'Prefix':path,\n",
    "        }\n",
    "        while next_token is not None:\n",
    "            kwargs = base_kwargs.copy()\n",
    "            if next_token != '':\n",
    "                kwargs.update({'ContinuationToken': next_token})\n",
    "            results = self.s3_client.list_objects_v2(**kwargs)\n",
    "            contents = results.get('Contents')\n",
    "            for i in contents:\n",
    "                k = i.get('Key')\n",
    "                if k[-1] != '/':\n",
    "                    keys.append(k)\n",
    "                else:\n",
    "                    dirs.append(k)\n",
    "                \n",
    "            next_token = results.get('NextContinuationToken')\n",
    "\n",
    "        for d in dirs:\n",
    "            dest_pathname = self.out_path.joinpath(d)\n",
    "            dest_pathname.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for k in tqdm(keys):\n",
    "            dest_pathname = self.out_path.joinpath(k)\n",
    "            dest_pathname.parent.mkdir(parents=True, exist_ok=True)\n",
    "            self.s3_client.download_file(self.bucket_name, k, str(dest_pathname))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Run conversion from Batch results to EOPatches\n",
    "\n",
    "Now all the parameters are set, we can run the conversion. For this step there are 2 available options:\n",
    "\n",
    "1. Download the contents of the Amazon S3 Bucket to a local file and convert locally to `EOPatches`\n",
    "2. Convert the data from the Amazon S3 Bucket and download the `EOPatches`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "First, we make an object, initialising it with the necessary parameters to perform the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make class to convert Batch to EOPatches\n",
    "batch2eolearn = ImportFromAWS(AWS_ID, AWS_SECRET, aws_bucket_name,\n",
    "                              request_id, band_names, timestamps, is_data=is_data_band, \n",
    "                              out_path=save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Option 1: Dowload Bucket and convert locally\n",
    "\n",
    "You can use the in-build function below to download your Amazon Bucket contents, or skip the cell and sync the Bucket using the aws cli tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download Batch results from Bucket\n",
    "batch2eolearn.download_from_bucket(obj_id=\"S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "If you specify the folder name containing the Batch request data in the `convert_to_EOPatches` function, the conversion will be performed locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert local data to EOPatches\n",
    "batch2eolearn.convert_to_EOPatches(local_bucket=f\"{save_folder}/{request_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Option 2: Convert in Bucket and download EOPatches\n",
    "\n",
    "The following cell allows you to convert the data that is stored on the Amazon S3 Bucket. Using the `save_locally` True/False flag you can either directly save the EOPatches on your computer or store them on the Amazon S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert Batch results in the Amazon Bucket to EoPatches\n",
    "# batch2eolearn.convert_to_EOPatches(save_locally=False)\n",
    "batch2eolearn.convert_to_EOPatches(save_locally=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "If you save the EOPatches to your Amazon Bucket, you can either download them by using the inbuilt command below or using an other method of your choice (e.g. `aws cli`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download EOPatches from Bucket\n",
    "batch2eolearn.download_from_bucket(obj_id=\"EO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Step 3: continue processing data using EOPatches\n",
    "\n",
    "Now we have the EOPatches saved locally, we can continue the process as shown in the [example EOLearn Notebook](https://eo-learn.readthedocs.io/en/latest/examples.html#land-use-and-land-cover).\n",
    "\n",
    "Before continuing with **Model construction and training** (see [here](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#6.-Model-construction-and-training)) we will need to import the reference data for Slovenia first. Since we are querying data over Slovenia, we need to fetch the reference data for the whole country [here](http://eo-learn.sentinel-hub.com/).\n",
    "\n",
    "For this example, the reference data prepared for Ljubljana can be downloaded [here](https://github.com/sentinel-hub/eo-learn/blob/master/examples/batch-processing/data/ljubljana_ref.gpkg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.1 Prepare the data to train the LULC prediction model\n",
    "\n",
    "#### 3.1.1 Setup reference data\n",
    "\n",
    "For this example, a subset of the country-wide reference for land-use-land-cover is provided. It is available in the form of a geopackage, which contains polygons and their corresponding labels. The labels represent the following 10 classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "- lulcid = 0, name = no data\n",
    "- lulcid = 1, name = cultivated land\n",
    "- lulcid = 2, name = forest\n",
    "- lulcid = 3, name = grassland\n",
    "- lulcid = 4, name = shrubland\n",
    "- lulcid = 5, name = water\n",
    "- lulcid = 6, name = wetlands\n",
    "- lulcid = 7, name = tundra\n",
    "- lulcid = 8, name = artificial surface\n",
    "- lulcid = 9, name = bareland\n",
    "- lulcid = 10, name = snow and ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LULC(MultiValueEnum):\n",
    "    \"\"\" Enum class containing basic LULC types\n",
    "    \"\"\"\n",
    "    NO_DATA            = 'No Data',            0,  '#ffffff'\n",
    "    CULTIVATED_LAND    = 'Cultivated Land',    1,  '#ffff00'\n",
    "    FOREST             = 'Forest',             2,  '#054907'\n",
    "    GRASSLAND          = 'Grassland',          3,  '#ffa500'\n",
    "    SHRUBLAND          = 'Shrubland',          4,  '#806000'\n",
    "    WATER              = 'Water',              5,  '#069af3'\n",
    "    WETLAND            = 'Wetlands',           6,  '#95d0fc'\n",
    "    TUNDRA             = 'Tundra',             7,  '#967bb6'\n",
    "    ARTIFICIAL_SURFACE = 'Artificial Surface', 8,  '#dc143c'\n",
    "    BARELAND           = 'Bareland',           9,  '#a6a6a6'\n",
    "    SNOW_AND_ICE       = 'Snow and Ice',       10, '#000000'\n",
    "\n",
    "    @property\n",
    "    def id(self):\n",
    "        \"\"\" Returns an ID of an enum type\n",
    "\n",
    "        :return: An ID\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.values[1]\n",
    "\n",
    "    @property\n",
    "    def color(self):\n",
    "        \"\"\" Returns class color\n",
    "\n",
    "        :return: A color in hexadecimal representation\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        return self.values[2]\n",
    "\n",
    "\n",
    "def get_bounds_from_ids(ids):\n",
    "    bounds = []\n",
    "    for i in range(len(ids)):\n",
    "        if i < len(ids) - 1:\n",
    "            if i == 0:\n",
    "                diff = (ids[i + 1] - ids[i]) / 2\n",
    "                bounds.append(ids[i] - diff)\n",
    "            diff = (ids[i + 1] - ids[i]) / 2\n",
    "            bounds.append(ids[i] + diff)\n",
    "        else:\n",
    "            diff = (ids[i] - ids[i - 1]) / 2\n",
    "            bounds.append(ids[i] + diff)\n",
    "    return bounds\n",
    "\n",
    "\n",
    "# Reference colormap things\n",
    "lulc_bounds = get_bounds_from_ids([x.id for x in LULC])\n",
    "lulc_cmap = ListedColormap([x.color for x in LULC], name=\"lulc_cmap\")\n",
    "lulc_norm = BoundaryNorm(lulc_bounds, lulc_cmap.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The main point of this task is to create a raster mask from the vector polygons and add it to the eopatch. With this procedure, any kind of a labeled shapefile can be transformed into a raster reference map. This result is achieved with the existing task `VectorToRaster` from the `eolearn.geometry package`. All polygons belonging to the each of the classes are separately burned to the raster mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This step takes some time due to the large size of the reference data (https://github.com/sentinel-hub/eo-learn/blob/master/examples/batch-processing/data/ljubljana_ref.gpkg)\n",
    "land_use_ref_path = INPUT_DATA.joinpath('ljubljana_ref.gpkg')\n",
    "\n",
    "land_use_ref = gpd.read_file(land_use_ref_path)\n",
    "\n",
    "rasterization_task = VectorToRaster(land_use_ref, (FeatureType.MASK_TIMELESS, 'LULC'),\n",
    "                                    values_column='lulcid', raster_shape=(FeatureType.MASK, 'IS_DATA'),\n",
    "                                    raster_dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 3.1.2 Define the workflow\n",
    "\n",
    "Now we want to incorporate the reference data to the EOPatches, we need to load and update the existing patches with the reference data (see [here](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#Reference-map-task)). We also want to perform the erosion step shown in the [EOLearn LULC example](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#5.-Prepare-the-training-data) in order to \"clean\" the data, as described in this [blog post](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-2-bd9aa86f8500).\n",
    "\n",
    "We will also prepare the model training data (see [here](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#5.-Prepare-the-training-data)) by performing random spatial sampling of the EOPatches, and finally split patches for training/validation.\n",
    "\n",
    "\n",
    "The following workflow is created and executed:\n",
    "\n",
    "1. Load the existing EOPatches containing satellite data\n",
    "2. Add rasterised reference data\n",
    "3. Perform erosion task on reference data\n",
    "4. Random spatial sampling of the EOPatches\n",
    "5. Split patches for training/validation\n",
    "6. Save updated EOPatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "First we set up the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task for LOAD\n",
    "load = LoadTask(f\"{save_folder}/EOPatches\")\n",
    "\n",
    "# TASK FOR EROSION\n",
    "# erode each class of the reference map\n",
    "erosion = ErosionTask(mask_feature=(FeatureType.MASK_TIMELESS,'LULC','LULC_ERODED'), disk_radius=1)\n",
    "\n",
    "# TASK FOR SPATIAL SAMPLING\n",
    "# Uniformly sample about pixels from patches\n",
    "n_samples = 500000 # half of pixels (1000*1000 patches here)\n",
    "ref_labels = list(range(11)) # reference labels to take into account when sampling\n",
    "spatial_sampling = PointSamplingTask(\n",
    "    n_samples=n_samples,\n",
    "    ref_mask_feature='LULC_ERODED',\n",
    "    ref_labels=ref_labels,\n",
    "    sample_features=[  # tag fields to sample\n",
    "        (FeatureType.DATA, 'FEATURES'),\n",
    "        (FeatureType.MASK_TIMELESS, 'LULC_ERODED')\n",
    "    ])\n",
    "\n",
    "# TASK FOR SAVING\n",
    "save = SaveTask(f\"{save_folder}/EOPatches\", overwrite_permission=OverwritePermission.OVERWRITE_PATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the workflow\n",
    "workflow = LinearWorkflow(load,\n",
    "                          rasterization_task,\n",
    "                          erosion,\n",
    "                          spatial_sampling,\n",
    "                          save,\n",
    "                         )\n",
    "\n",
    "# Let's visualize it\n",
    "workflow.dependency_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 3.1.3 Run the EOWorkflow over all EOPatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "   \n",
    "execution_args = []\n",
    "for patch in Path(save_folder).joinpath(\"EOPatches\").iterdir():\n",
    "     execution_args.append({\n",
    "         load: {'eopatch_folder': patch.name},\n",
    "         spatial_sampling: {'seed': 42},\n",
    "         save: {'eopatch_folder': patch.name}\n",
    "    })\n",
    "\n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
    "executor.run(workers=1, multiprocess=False)\n",
    "\n",
    "executor.make_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.2 Visualize the patches\n",
    "\n",
    "Now we have EOPatches ready for analysis, we can visualise them before continuing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a random patch\n",
    "patch_ID = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**RGB True Color of a Patch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,figsize=(12,12))\n",
    "eopatch = EOPatch.load(f'{save_folder}/EOPatches/eopatch_{patch_ID}', lazy_loading=True)\n",
    "eopatch\n",
    "ax.imshow(np.clip(eopatch.data['FEATURES'][1][..., [2, 1, 0]] * 3.5, 0, 1))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_aspect(1)\n",
    "plt.show()\n",
    "eopatch = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Temporal mean of NDVI of a patch**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "eopatch = EOPatch.load(f'{save_folder}/EOPatches/eopatch_{patch_ID}', lazy_loading=True)\n",
    "ndvi = eopatch.data['FEATURES'][:, :, :, 6]\n",
    "mask = eopatch.mask['IS_DATA'].squeeze()\n",
    "ndvi[mask==0] = np.nan\n",
    "ndvi_mean = np.nanmean(ndvi, axis=0).squeeze()\n",
    "im = ax.imshow(ndvi_mean, vmin=0, vmax=0.8, cmap=plt.get_cmap('YlGn'))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_aspect(1)\n",
    "eopatch = None\n",
    "\n",
    "cb = fig.colorbar(im, ax=ax, orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Reference data example of a Patch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "eopatch = EOPatch.load(f'{save_folder}/EOPatches/eopatch_{patch_ID}', lazy_loading=True)\n",
    "\n",
    "ax.imshow(eopatch.mask_timeless['LULC'].squeeze(), cmap=lulc_cmap, norm=lulc_norm)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_aspect(1)\n",
    "eopatch = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Note:\n",
    "\n",
    "At this point, we have a setup similar to what we would have at [Section 6](https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html#6.-Model-construction-and-training) of the eo-learn LULC tutorial. If you are familiar with the process you can stop here, or you can continue the steps described in the [the eo-learn example](https://github.com/sentinel-hub/eo-learn/blob/master/examples/land-cover-map/SI_LULC_pipeline.ipynb) Notebook from here on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDC 0.22.3 (Python3)",
   "language": "python",
   "name": "edc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.284041,
   "end_time": "2021-02-10T14:15:08.873075",
   "environment_variables": {},
   "exception": true,
   "input_path": "/tmp/tmplhp5xynd",
   "output_path": "/tmp/notebook_output.ipynb",
   "parameters": {},
   "start_time": "2021-02-10T14:15:02.589034",
   "version": "2.2.2"
  },
  "properties": {
   "authors": [
    {
     "id": "a2b85af8-6b99-4fa2-acf6-e87d74e40431",
     "name": "maxim.lamare@sinergise.com"
    }
   ],
   "description": "Replace part of the eo-learn workflow with Batch Processing",
   "id": "3c264153-2bfc-42fa-9a7a-2276c88755f8",
   "license": null,
   "name": "Batch Processing with eo-learn",
   "requirements": [
    "eurodatacube",
    "eoxhub"
   ],
   "tags": [
    "EO Data",
    "Jupyter",
    "Machine Learning",
    "Land-Use-Classification",
    "Mass Processing",
    "Sentinel Data",
    "Sentinel Hub"
   ],
   "tosAgree": true,
   "type": "Jupyter Notebook",
   "version": "0.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}